{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e372ce03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 11:21:16.859809: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-18 11:21:16.897712: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-11-18 11:21:16.897744: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-11-18 11:21:16.897772: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-18 11:21:16.904908: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-18 11:21:17.701004: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jiachenhu/miniconda3/envs/eqp/lib/python3.11/site-packages/tensorflow/python/compat/v2_compat.py:108: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf1\n",
    "tf1.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e4cafee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import glob\n",
    "import math\n",
    "import os\n",
    "import shutil\n",
    "from scipy import signal\n",
    "from scipy.signal import butter, lfilter\n",
    "from tensorflow.keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D, Conv1D, MaxPooling1D, UpSampling1D, Flatten, Dropout, Reshape \n",
    "from tensorflow.keras.layers import Bidirectional, concatenate, BatchNormalization, ZeroPadding1D, Conv2DTranspose\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler, ReduceLROnPlateau, EarlyStopping\n",
    "from tensorflow.keras.initializers import VarianceScaling\n",
    "from tensorflow.keras.callbacks import CSVLogger\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import savefig\n",
    "from tensorflow.python.keras.layers import Layer, InputSpec\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.reset_default_graph()\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from keras.callbacks import Callback, EarlyStopping, ModelCheckpoint\n",
    "from keras.layers import Activation, Add, Bidirectional, Conv1D, Dense, Dropout, Embedding, Flatten, Reshape, multiply\n",
    "from keras.layers import concatenate, GRU, Input, LSTM, MaxPooling1D\n",
    "from keras.layers import GlobalAveragePooling1D,  GlobalMaxPooling1D, SpatialDropout1D\n",
    "from keras.models import Model\n",
    "from keras.preprocessing import text, sequence\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, log_loss\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers, callbacks\n",
    "from keras import backend as K\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "429e1139",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(x, hidden_units, dropout_rate):\n",
    "    for units in hidden_units:\n",
    "        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n",
    "        #x = layers.Dense(units, activation='relu')(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "065e6343",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_rate = 0.2\n",
    "stochastic_depth_rate = 0.1\n",
    "\n",
    "w1 = 100\n",
    "\n",
    "positional_emb = False\n",
    "conv_layers = 2\n",
    "num_classes = 1\n",
    "input_shape = (600,1)\n",
    "image_size = 600  # We'll resize input images to this size\n",
    "projection_dim = int(2*w1)\n",
    "num_heads = 4\n",
    "transformer_units = [\n",
    "    projection_dim,\n",
    "    projection_dim,\n",
    "]  # Size of the transformer layers\n",
    "transformer_layers = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4026302",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "class CCTTokenizer1(layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        kernel_size=4,\n",
    "        stride=1,\n",
    "        padding=1,\n",
    "        pooling_kernel_size=3,\n",
    "        pooling_stride=(2,2,2,2,2,2,2,2),\n",
    "        num_conv_layers=conv_layers,\n",
    "        num_output_channels=[int(projection_dim), int(projection_dim), int(projection_dim), int(projection_dim), int(projection_dim), int(projection_dim), int(projection_dim), int(projection_dim)],\n",
    "        positional_emb=positional_emb,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super(CCTTokenizer1, self).__init__(**kwargs)\n",
    "\n",
    "        # This is our tokenizer.\n",
    "        self.conv_model = tf.keras.Sequential()\n",
    "        for i in range(num_conv_layers):\n",
    "            self.conv_model.add(\n",
    "                layers.Conv1D(\n",
    "                    num_output_channels[i],\n",
    "                    kernel_size,\n",
    "                    stride,\n",
    "                    padding=\"same\",\n",
    "                    use_bias=False,\n",
    "                    activation=\"relu\",\n",
    "                    kernel_initializer=\"he_normal\",\n",
    "                )\n",
    "            )\n",
    "            #self.conv_model.add(layers.ZeroPadding2D(padding))\n",
    "            self.conv_model.add(\n",
    "                layers.MaxPool1D(pooling_kernel_size, (pooling_stride[i]), \"same\")\n",
    "            )\n",
    "\n",
    "        self.positional_emb = positional_emb\n",
    "\n",
    "    def call(self, images):\n",
    "        outputs = self.conv_model(images)\n",
    "        # After passing the images through our mini-network the spatial dimensions\n",
    "        # are flattened to form sequences.\n",
    "        reshaped = tf.reshape(\n",
    "            outputs,\n",
    "            (-1, tf.shape(outputs)[1], tf.shape(outputs)[-1]),\n",
    "        )\n",
    "        return reshaped\n",
    "\n",
    "    def positional_embedding(self, image_size):\n",
    "        # Positional embeddings are optional in CCT. Here, we calculate\n",
    "        # the number of sequences and initialize an `Embedding` layer to\n",
    "        # compute the positional embeddings later.\n",
    "        if self.positional_emb:\n",
    "            dummy_inputs = tf.ones((1, image_size, 1))\n",
    "            dummy_outputs = self.call(dummy_inputs)\n",
    "            sequence_length = dummy_outputs.shape[1]\n",
    "            projection_dim = dummy_outputs.shape[-1]\n",
    "\n",
    "            print(dummy_outputs,sequence_length,projection_dim)\n",
    "            embed_layer = layers.Embedding(\n",
    "                input_dim=sequence_length, output_dim=projection_dim\n",
    "            )\n",
    "            return embed_layer, sequence_length\n",
    "        else:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bdabf35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Referred from: github.com:rwightman/pytorch-image-models.\n",
    "class StochasticDepth(layers.Layer):\n",
    "    def __init__(self, drop_prop, **kwargs):\n",
    "        super(StochasticDepth, self).__init__(**kwargs)\n",
    "        self.drop_prob = drop_prop\n",
    "\n",
    "    def call(self, x, training=None):\n",
    "        if training:\n",
    "            keep_prob = 1 - self.drop_prob\n",
    "            shape = (tf.shape(x)[0],) + (1,) * (len(tf.shape(x)) - 1)\n",
    "            random_tensor = keep_prob + tf.random.uniform(shape, 0, 1)\n",
    "            random_tensor = tf.floor(random_tensor)\n",
    "            return (x / keep_prob) * random_tensor\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0db68754",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def create_vit_classifier(inputs):\n",
    "def create_cct_model1(inputs):\n",
    "\n",
    "\n",
    "    # Augment data.\n",
    "    #augmented = data_augmentation(inputs)\n",
    "\n",
    "    # Encode patches.\n",
    "    cct_tokenizer = CCTTokenizer1()\n",
    "    encoded_patches = cct_tokenizer(inputs)\n",
    "\n",
    "    # Apply positional embedding.\n",
    "    if positional_emb:\n",
    "        pos_embed, seq_length = cct_tokenizer.positional_embedding(image_size)\n",
    "        positions = tf.range(start=0, limit=seq_length, delta=1)\n",
    "        position_embeddings = pos_embed(positions)\n",
    "        encoded_patches += position_embeddings\n",
    "\n",
    "    # Calculate Stochastic Depth probabilities.\n",
    "    dpr = [x for x in np.linspace(0, stochastic_depth_rate, transformer_layers)]\n",
    "\n",
    "    # Create multiple layers of the Transformer block.\n",
    "    for i in range(transformer_layers):\n",
    "        # Layer normalization 1.\n",
    "        x1 = layers.LayerNormalization(epsilon=1e-5)(encoded_patches)\n",
    "\n",
    "        # Create a multi-head attention layer.\n",
    "        attention_output = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=projection_dim, dropout=0.2\n",
    "        )(x1, x1)\n",
    "\n",
    "        #print(encoded_patches)\n",
    "        # Skip connection 1.\n",
    "        attention_output = StochasticDepth(dpr[i])(attention_output)\n",
    "        x2 = layers.Add()([attention_output, encoded_patches])\n",
    "\n",
    "        # Layer normalization 2.\n",
    "        x3 = layers.LayerNormalization(epsilon=1e-5)(x2)\n",
    "        #x3 = x2\n",
    "        \n",
    "        # MLP.\n",
    "        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.2)\n",
    "\n",
    "        # Skip connection 2.\n",
    "        #print(x3)\n",
    "        x3 = StochasticDepth(dpr[i])(x3)\n",
    "        #print(x3)\n",
    "        encoded_patches = layers.Add()([x3, x2])\n",
    "     \n",
    "    # Apply sequence pooling.\n",
    "    representation = layers.LayerNormalization(epsilon=1e-5)(encoded_patches)\n",
    "    \n",
    "    ''' \n",
    "    attention_weights = tf.nn.softmax(layers.Dense(1)(representation), axis=1)\n",
    "    weighted_representation = tf.matmul(\n",
    "        attention_weights, representation, transpose_a=True\n",
    "    )\n",
    "    weighted_representation = tf.squeeze(weighted_representation, -2)\n",
    "    '''\n",
    "    return representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa3e1220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jiachenhu/miniconda3/envs/eqp/lib/python3.11/site-packages/tensorflow/python/ops/init_ops.py:94: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/jiachenhu/miniconda3/envs/eqp/lib/python3.11/site-packages/tensorflow/python/ops/init_ops.py:94: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input (InputLayer)          [(None, 600, 1)]             0         []                            \n",
      "                                                                                                  \n",
      " cct_tokenizer1 (CCTTokeniz  (None, 150, 200)             160800    ['input[0][0]']               \n",
      " er1)                                                                                             \n",
      "                                                                                                  \n",
      " layer_normalization (Layer  (None, 150, 200)             400       ['cct_tokenizer1[0][0]']      \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " multi_head_attention (Mult  (None, 150, 200)             642600    ['layer_normalization[0][0]', \n",
      " iHeadAttention)                                                     'layer_normalization[0][0]'] \n",
      "                                                                                                  \n",
      " stochastic_depth (Stochast  (None, 150, 200)             0         ['multi_head_attention[0][0]']\n",
      " icDepth)                                                                                         \n",
      "                                                                                                  \n",
      " add (Add)                   (None, 150, 200)             0         ['stochastic_depth[0][0]',    \n",
      "                                                                     'cct_tokenizer1[0][0]']      \n",
      "                                                                                                  \n",
      " layer_normalization_1 (Lay  (None, 150, 200)             400       ['add[0][0]']                 \n",
      " erNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 150, 200)             40200     ['layer_normalization_1[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)         (None, 150, 200)             0         ['dense[0][0]']               \n",
      "                                                                                                  \n",
      " dense_1 (Dense)             (None, 150, 200)             40200     ['dropout_1[0][0]']           \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)         (None, 150, 200)             0         ['dense_1[0][0]']             \n",
      "                                                                                                  \n",
      " stochastic_depth_1 (Stocha  (None, 150, 200)             0         ['dropout_2[0][0]']           \n",
      " sticDepth)                                                                                       \n",
      "                                                                                                  \n",
      " add_1 (Add)                 (None, 150, 200)             0         ['stochastic_depth_1[0][0]',  \n",
      "                                                                     'add[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_2 (Lay  (None, 150, 200)             400       ['add_1[0][0]']               \n",
      " erNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " multi_head_attention_1 (Mu  (None, 150, 200)             642600    ['layer_normalization_2[0][0]'\n",
      " ltiHeadAttention)                                                  , 'layer_normalization_2[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " stochastic_depth_2 (Stocha  (None, 150, 200)             0         ['multi_head_attention_1[0][0]\n",
      " sticDepth)                                                         ']                            \n",
      "                                                                                                  \n",
      " add_2 (Add)                 (None, 150, 200)             0         ['stochastic_depth_2[0][0]',  \n",
      "                                                                     'add_1[0][0]']               \n",
      "                                                                                                  \n",
      " layer_normalization_3 (Lay  (None, 150, 200)             400       ['add_2[0][0]']               \n",
      " erNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " dense_2 (Dense)             (None, 150, 200)             40200     ['layer_normalization_3[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)         (None, 150, 200)             0         ['dense_2[0][0]']             \n",
      "                                                                                                  \n",
      " dense_3 (Dense)             (None, 150, 200)             40200     ['dropout_4[0][0]']           \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)         (None, 150, 200)             0         ['dense_3[0][0]']             \n",
      "                                                                                                  \n",
      " stochastic_depth_3 (Stocha  (None, 150, 200)             0         ['dropout_5[0][0]']           \n",
      " sticDepth)                                                                                       \n",
      "                                                                                                  \n",
      " add_3 (Add)                 (None, 150, 200)             0         ['stochastic_depth_3[0][0]',  \n",
      "                                                                     'add_2[0][0]']               \n",
      "                                                                                                  \n",
      " layer_normalization_4 (Lay  (None, 150, 200)             400       ['add_3[0][0]']               \n",
      " erNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " multi_head_attention_2 (Mu  (None, 150, 200)             642600    ['layer_normalization_4[0][0]'\n",
      " ltiHeadAttention)                                                  , 'layer_normalization_4[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " stochastic_depth_4 (Stocha  (None, 150, 200)             0         ['multi_head_attention_2[0][0]\n",
      " sticDepth)                                                         ']                            \n",
      "                                                                                                  \n",
      " add_4 (Add)                 (None, 150, 200)             0         ['stochastic_depth_4[0][0]',  \n",
      "                                                                     'add_3[0][0]']               \n",
      "                                                                                                  \n",
      " layer_normalization_5 (Lay  (None, 150, 200)             400       ['add_4[0][0]']               \n",
      " erNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " dense_4 (Dense)             (None, 150, 200)             40200     ['layer_normalization_5[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)         (None, 150, 200)             0         ['dense_4[0][0]']             \n",
      "                                                                                                  \n",
      " dense_5 (Dense)             (None, 150, 200)             40200     ['dropout_7[0][0]']           \n",
      "                                                                                                  \n",
      " dropout_8 (Dropout)         (None, 150, 200)             0         ['dense_5[0][0]']             \n",
      "                                                                                                  \n",
      " stochastic_depth_5 (Stocha  (None, 150, 200)             0         ['dropout_8[0][0]']           \n",
      " sticDepth)                                                                                       \n",
      "                                                                                                  \n",
      " add_5 (Add)                 (None, 150, 200)             0         ['stochastic_depth_5[0][0]',  \n",
      "                                                                     'add_4[0][0]']               \n",
      "                                                                                                  \n",
      " layer_normalization_6 (Lay  (None, 150, 200)             400       ['add_5[0][0]']               \n",
      " erNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " multi_head_attention_3 (Mu  (None, 150, 200)             642600    ['layer_normalization_6[0][0]'\n",
      " ltiHeadAttention)                                                  , 'layer_normalization_6[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " stochastic_depth_6 (Stocha  (None, 150, 200)             0         ['multi_head_attention_3[0][0]\n",
      " sticDepth)                                                         ']                            \n",
      "                                                                                                  \n",
      " add_6 (Add)                 (None, 150, 200)             0         ['stochastic_depth_6[0][0]',  \n",
      "                                                                     'add_5[0][0]']               \n",
      "                                                                                                  \n",
      " layer_normalization_7 (Lay  (None, 150, 200)             400       ['add_6[0][0]']               \n",
      " erNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " dense_6 (Dense)             (None, 150, 200)             40200     ['layer_normalization_7[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " dropout_10 (Dropout)        (None, 150, 200)             0         ['dense_6[0][0]']             \n",
      "                                                                                                  \n",
      " dense_7 (Dense)             (None, 150, 200)             40200     ['dropout_10[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_11 (Dropout)        (None, 150, 200)             0         ['dense_7[0][0]']             \n",
      "                                                                                                  \n",
      " stochastic_depth_7 (Stocha  (None, 150, 200)             0         ['dropout_11[0][0]']          \n",
      " sticDepth)                                                                                       \n",
      "                                                                                                  \n",
      " add_7 (Add)                 (None, 150, 200)             0         ['stochastic_depth_7[0][0]',  \n",
      "                                                                     'add_6[0][0]']               \n",
      "                                                                                                  \n",
      " layer_normalization_8 (Lay  (None, 150, 200)             400       ['add_7[0][0]']               \n",
      " erNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " flatten (Flatten)           (None, 30000)                0         ['layer_normalization_8[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " dense_8 (Dense)             (None, 1)                    30001     ['flatten[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3086401 (11.77 MB)\n",
      "Trainable params: 3086401 (11.77 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = layers.Input(shape=input_shape,name='input')\n",
    "\n",
    "featuresP = create_cct_model1(inputs)\n",
    "#featuresP = layers.Dropout(0.2)(featuresP)\n",
    "featuresP = layers.Flatten()(featuresP)\n",
    "logitp = layers.Dense(1, activation='sigmoid')(featuresP)\n",
    "\n",
    "\n",
    "#logitp  = Conv2D(1,  3, strides =(1), padding='same',activation='sigmoid', kernel_initializer='he_normal',name='picker_P')(featuresP)\n",
    "\n",
    "\n",
    "model = Model(inputs=[inputs], outputs=[logitp])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1417175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input (InputLayer)          [(None, 600, 1)]             0         []                            \n",
      "                                                                                                  \n",
      " cct_tokenizer1_1 (CCTToken  (None, 150, 200)             160800    ['input[0][0]']               \n",
      " izer1)                                                                                           \n",
      "                                                                                                  \n",
      " layer_normalization_9 (Lay  (None, 150, 200)             400       ['cct_tokenizer1_1[0][0]']    \n",
      " erNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " multi_head_attention_4 (Mu  (None, 150, 200)             642600    ['layer_normalization_9[0][0]'\n",
      " ltiHeadAttention)                                                  , 'layer_normalization_9[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " stochastic_depth_8 (Stocha  (None, 150, 200)             0         ['multi_head_attention_4[0][0]\n",
      " sticDepth)                                                         ']                            \n",
      "                                                                                                  \n",
      " add_8 (Add)                 (None, 150, 200)             0         ['stochastic_depth_8[0][0]',  \n",
      "                                                                     'cct_tokenizer1_1[0][0]']    \n",
      "                                                                                                  \n",
      " layer_normalization_10 (La  (None, 150, 200)             400       ['add_8[0][0]']               \n",
      " yerNormalization)                                                                                \n",
      "                                                                                                  \n",
      " dense_9 (Dense)             (None, 150, 200)             40200     ['layer_normalization_10[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_13 (Dropout)        (None, 150, 200)             0         ['dense_9[0][0]']             \n",
      "                                                                                                  \n",
      " dense_10 (Dense)            (None, 150, 200)             40200     ['dropout_13[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_14 (Dropout)        (None, 150, 200)             0         ['dense_10[0][0]']            \n",
      "                                                                                                  \n",
      " stochastic_depth_9 (Stocha  (None, 150, 200)             0         ['dropout_14[0][0]']          \n",
      " sticDepth)                                                                                       \n",
      "                                                                                                  \n",
      " add_9 (Add)                 (None, 150, 200)             0         ['stochastic_depth_9[0][0]',  \n",
      "                                                                     'add_8[0][0]']               \n",
      "                                                                                                  \n",
      " layer_normalization_11 (La  (None, 150, 200)             400       ['add_9[0][0]']               \n",
      " yerNormalization)                                                                                \n",
      "                                                                                                  \n",
      " multi_head_attention_5 (Mu  (None, 150, 200)             642600    ['layer_normalization_11[0][0]\n",
      " ltiHeadAttention)                                                  ',                            \n",
      "                                                                     'layer_normalization_11[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " stochastic_depth_10 (Stoch  (None, 150, 200)             0         ['multi_head_attention_5[0][0]\n",
      " asticDepth)                                                        ']                            \n",
      "                                                                                                  \n",
      " add_10 (Add)                (None, 150, 200)             0         ['stochastic_depth_10[0][0]', \n",
      "                                                                     'add_9[0][0]']               \n",
      "                                                                                                  \n",
      " layer_normalization_12 (La  (None, 150, 200)             400       ['add_10[0][0]']              \n",
      " yerNormalization)                                                                                \n",
      "                                                                                                  \n",
      " dense_11 (Dense)            (None, 150, 200)             40200     ['layer_normalization_12[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_16 (Dropout)        (None, 150, 200)             0         ['dense_11[0][0]']            \n",
      "                                                                                                  \n",
      " dense_12 (Dense)            (None, 150, 200)             40200     ['dropout_16[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_17 (Dropout)        (None, 150, 200)             0         ['dense_12[0][0]']            \n",
      "                                                                                                  \n",
      " stochastic_depth_11 (Stoch  (None, 150, 200)             0         ['dropout_17[0][0]']          \n",
      " asticDepth)                                                                                      \n",
      "                                                                                                  \n",
      " add_11 (Add)                (None, 150, 200)             0         ['stochastic_depth_11[0][0]', \n",
      "                                                                     'add_10[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_13 (La  (None, 150, 200)             400       ['add_11[0][0]']              \n",
      " yerNormalization)                                                                                \n",
      "                                                                                                  \n",
      " multi_head_attention_6 (Mu  (None, 150, 200)             642600    ['layer_normalization_13[0][0]\n",
      " ltiHeadAttention)                                                  ',                            \n",
      "                                                                     'layer_normalization_13[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " stochastic_depth_12 (Stoch  (None, 150, 200)             0         ['multi_head_attention_6[0][0]\n",
      " asticDepth)                                                        ']                            \n",
      "                                                                                                  \n",
      " add_12 (Add)                (None, 150, 200)             0         ['stochastic_depth_12[0][0]', \n",
      "                                                                     'add_11[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_14 (La  (None, 150, 200)             400       ['add_12[0][0]']              \n",
      " yerNormalization)                                                                                \n",
      "                                                                                                  \n",
      " dense_13 (Dense)            (None, 150, 200)             40200     ['layer_normalization_14[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_19 (Dropout)        (None, 150, 200)             0         ['dense_13[0][0]']            \n",
      "                                                                                                  \n",
      " dense_14 (Dense)            (None, 150, 200)             40200     ['dropout_19[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_20 (Dropout)        (None, 150, 200)             0         ['dense_14[0][0]']            \n",
      "                                                                                                  \n",
      " stochastic_depth_13 (Stoch  (None, 150, 200)             0         ['dropout_20[0][0]']          \n",
      " asticDepth)                                                                                      \n",
      "                                                                                                  \n",
      " add_13 (Add)                (None, 150, 200)             0         ['stochastic_depth_13[0][0]', \n",
      "                                                                     'add_12[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_15 (La  (None, 150, 200)             400       ['add_13[0][0]']              \n",
      " yerNormalization)                                                                                \n",
      "                                                                                                  \n",
      " multi_head_attention_7 (Mu  (None, 150, 200)             642600    ['layer_normalization_15[0][0]\n",
      " ltiHeadAttention)                                                  ',                            \n",
      "                                                                     'layer_normalization_15[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " stochastic_depth_14 (Stoch  (None, 150, 200)             0         ['multi_head_attention_7[0][0]\n",
      " asticDepth)                                                        ']                            \n",
      "                                                                                                  \n",
      " add_14 (Add)                (None, 150, 200)             0         ['stochastic_depth_14[0][0]', \n",
      "                                                                     'add_13[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_16 (La  (None, 150, 200)             400       ['add_14[0][0]']              \n",
      " yerNormalization)                                                                                \n",
      "                                                                                                  \n",
      " dense_15 (Dense)            (None, 150, 200)             40200     ['layer_normalization_16[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_22 (Dropout)        (None, 150, 200)             0         ['dense_15[0][0]']            \n",
      "                                                                                                  \n",
      " dense_16 (Dense)            (None, 150, 200)             40200     ['dropout_22[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_23 (Dropout)        (None, 150, 200)             0         ['dense_16[0][0]']            \n",
      "                                                                                                  \n",
      " stochastic_depth_15 (Stoch  (None, 150, 200)             0         ['dropout_23[0][0]']          \n",
      " asticDepth)                                                                                      \n",
      "                                                                                                  \n",
      " add_15 (Add)                (None, 150, 200)             0         ['stochastic_depth_15[0][0]', \n",
      "                                                                     'add_14[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_17 (La  (None, 150, 200)             400       ['add_15[0][0]']              \n",
      " yerNormalization)                                                                                \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)         (None, 30000)                0         ['layer_normalization_17[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_24 (Dropout)        (None, 30000)                0         ['flatten_1[0][0]']           \n",
      "                                                                                                  \n",
      " dense_17 (Dense)            (None, 1)                    30001     ['dropout_24[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3086401 (11.77 MB)\n",
      "Trainable params: 3086401 (11.77 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = layers.Input(shape=input_shape,name='input')\n",
    "\n",
    "featuresP = create_cct_model1(inputs)\n",
    "featuresP = layers.Flatten()(featuresP)\n",
    "featuresP = layers.Dropout(0.2)(featuresP)\n",
    "logitp = layers.Dense(1, activation='sigmoid')(featuresP)\n",
    "\n",
    "\n",
    "#logitp  = Conv2D(1,  3, strides =(1), padding='same',activation='sigmoid', kernel_initializer='he_normal',name='picker_P')(featuresP)\n",
    "\n",
    "\n",
    "model = Model(inputs=[inputs], outputs=[logitp])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b44e5b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('../../../model/Toc2me_20240819_Transfer_Learning_21916data/best_weigths_Binary_Toc2me_Transfer_SCSN_20241013_21916data_90.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ce106d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../all_data_demo/20161104064824.680 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_869357/3635558723.py:41: RuntimeWarning: invalid value encountered in divide\n",
      "  dat = dat / np.max(np.abs(dat))\n",
      "/home/jiachenhu/miniconda3/envs/eqp/lib/python3.11/site-packages/keras/src/engine/training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n",
      "2024-11-18 11:24:42.879754: W tensorflow/c/c_api.cc:305] Operation '{name:'dense_17/Sigmoid' id:2513 op device:{requested: '', assigned: ''} def:{{{node dense_17/Sigmoid}} = Sigmoid[T=DT_FLOAT, _has_manual_control_dependencies=true](dense_17/BiasAdd)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../all_data_demo/20161125051408.940 2/3\n",
      "../all_data_demo/20161128051644.670 3/3\n"
     ]
    }
   ],
   "source": [
    "from obspy.core.event import read_events\n",
    "import os\n",
    "import obspy\n",
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "datadir = '../all_data_demo'\n",
    "evtdirs = []\n",
    "with open(os.path.join(datadir, 'catalog_sorted_ToC2ME.txt'), 'r') as f:\n",
    "    lines = f.readlines()\n",
    "for line in lines:\n",
    "    evtdirs.append(os.path.join(datadir, line.strip()))\n",
    "\n",
    "for i, evtdir in enumerate(evtdirs):\n",
    "    print('{0:s} {1:d}/{2:d}'.format(evtdir, i+1, len(evtdirs)))\n",
    "    polall = []\n",
    "    datall = []        \n",
    "    evlas = []\n",
    "    evlos = []\n",
    "    evdps = []\n",
    "    stlas = []\n",
    "    stlos = []\n",
    "    stalist = []\n",
    "    \n",
    "    # read waveforms\n",
    "    sacfiles = np.sort(glob.glob(os.path.join(evtdir, '*DHZ.SAC')))\n",
    "    for j, sacfile in enumerate(sacfiles):\n",
    "        try:\n",
    "            st = obspy.read(sacfile)\n",
    "        except TypeError:\n",
    "            print(f'Skipped wrong file: {sacfile}')\n",
    "            continue\n",
    "        \n",
    "        t = st[0].stats.starttime + 5\n",
    "        st.trim(t - 0.6 + 0.002, t + 0.6)\n",
    "        dat = st[0].data\n",
    "\n",
    "        if len(dat) == 600:\n",
    "            dat = dat / np.max(np.abs(dat))\n",
    "            dat = np.reshape(dat, (dat.shape[0], 1))\n",
    "            datall.append(dat)\n",
    "            \n",
    "            stalist.append(st[0].stats.sac.kstnm)\n",
    "            stlas.append(st[0].stats.sac.stla)\n",
    "            stlos.append(st[0].stats.sac.stlo)            \n",
    "            evlas.append(st[0].stats.sac.evla)\n",
    "            evlos.append(st[0].stats.sac.evlo)\n",
    "            evdps.append(st[0].stats.sac.evdp)\n",
    "            \n",
    "    if len(datall) == 0:\n",
    "        print(f\"No valid data for directory: {evtdir}\")\n",
    "        continue\n",
    "\n",
    "    datall = np.array(datall)\n",
    "\n",
    "    try:\n",
    "        if datall.ndim != 3 or datall.shape[1] != 600 or datall.shape[2] != 1:\n",
    "            raise ValueError(f\"Invalid input shape: {datall.shape}\")\n",
    "        out = model.predict(datall, batch_size=1024, verbose=1)\n",
    "        pols = [pol[0] for pol in out]\n",
    "    except ValueError as e:\n",
    "        print(f\"Skipped wrong input shape: {e}\")\n",
    "        continue\n",
    "\n",
    "    # save model prediction\n",
    "    outdir = './result_ToC2ME_eqpolarity'\n",
    "    filename = evtdir.split('/')[-1] + '.polarity'\n",
    "    with open(os.path.join(outdir, filename), 'w') as f:\n",
    "        for sta, evla, evlo, evdp, stla, stlo, pol in zip(stalist, evlas, evlos, evdps, stlas, stlos, pols):\n",
    "            f.write('{0:s} {1:f} {2:f} {3:f} {4:f} {5:f} {6:f}\\n'.format(sta, evla, evlo, evdp, stla, stlo, pol))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cafc290",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(len(datall)):\n",
    "    plt.figure()\n",
    "    plt.plot(datall[idx])\n",
    "    plt.xlim([200,400])\n",
    "    plt.title('{0:s} Prob={1:f}'.format(stalist[idx],float(out[idx])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
